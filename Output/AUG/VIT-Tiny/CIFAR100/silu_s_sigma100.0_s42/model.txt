_parameters: {}
_buffers: {}
_non_persistent_buffers_set: set()
_backward_pre_hooks: OrderedDict()
_backward_hooks: OrderedDict()
_is_full_backward_hook: None
_forward_hooks: OrderedDict()
_forward_hooks_with_kwargs: OrderedDict()
_forward_hooks_always_called: OrderedDict()
_forward_pre_hooks: OrderedDict()
_forward_pre_hooks_with_kwargs: OrderedDict()
_state_dict_hooks: OrderedDict()
_state_dict_pre_hooks: OrderedDict()
_load_state_dict_pre_hooks: OrderedDict()
_load_state_dict_post_hooks: OrderedDict()
_modules: {'_orig_mod': ViT(
  (patch_embedding): PatchEmbedding(
    (linear_projection): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (flatten): Flatten(start_dim=2, end_dim=-1)
  )
  (positional_encoding): PositionalEncoding()
  (transformer_encoder): Sequential(
    (0): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (1): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (2): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (3): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (4): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (5): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (6): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (7): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (8): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (9): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (10): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (11): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
  )
  (classifier): Linear(in_features=192, out_features=100, bias=True)
)}
dynamo_ctx: <torch._dynamo.eval_frame.OptimizeContext object at 0x14b547f44710>
forward: <function Module._wrapped_call_impl at 0x14b4e73c7380>
_torchdynamo_orig_callable: <bound method ViT.forward of ViT(
  (patch_embedding): PatchEmbedding(
    (linear_projection): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (flatten): Flatten(start_dim=2, end_dim=-1)
  )
  (positional_encoding): PositionalEncoding()
  (transformer_encoder): Sequential(
    (0): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (1): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (2): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (3): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (4): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (5): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (6): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (7): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (8): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (9): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (10): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
    (11): TransformerEncoder(
      (attention): MultiHeadAttention(
        (dropout): Dropout(p=0.1, inplace=False)
        (W_q): Linear(in_features=192, out_features=192, bias=False)
        (W_k): Linear(in_features=192, out_features=192, bias=False)
        (W_v): Linear(in_features=192, out_features=192, bias=False)
        (W_o): Linear(in_features=192, out_features=192, bias=True)
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
      (mlp): Sequential(
        (0): Linear(in_features=192, out_features=768, bias=True)
        (1): SiLU_s()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=768, out_features=192, bias=True)
      )
    )
  )
  (classifier): Linear(in_features=192, out_features=100, bias=True)
)>
get_compiler_config: <function _TorchDynamoContext.__call__.<locals>.get_compiler_config at 0x14b554e0bba0>
