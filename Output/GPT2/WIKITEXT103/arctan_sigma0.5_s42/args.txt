vocab_size: 50257
max_seq_length: 1024
embedding_dim: 768
num_attention_heads: 12
num_layers: 12
dropout: 0.1
activation: arctan
sigma: 0.5
inplace: True
dataset: wikitext103
data_path: ./Data
compile: False
compile_mode: default
batch_size: 32
num_epochs: 20
use_amp: True
clip_grad_norm: 1.0
num_workers: 14
persistent_workers: False
prefetch_factor: None
pin_memory: True
optimizer: adamw
weight_decay: 0.1
lr: 0.0006
scheduler: linear
lr_step: 20
lr_gamma: 0.5
device: cuda
seed: 42
output_dir: ./Output/GPT2/WIKITEXT103/arctan_sigma0.5_s42
test_only: False
ddp: False
ddp_batch_size: 32
total_params: 124439808
trainable_params: 124439808
