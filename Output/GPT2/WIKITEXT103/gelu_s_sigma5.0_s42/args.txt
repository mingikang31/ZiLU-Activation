vocab_size: 50257
max_seq_length: 1024
embedding_dim: 768
num_attention_heads: 12
num_layers: 12
dropout: 0.1
activation: gelu_s
sigma: 5.0
inplace: True
dataset: wikitext103
data_path: ./Data
compile: False
compile_mode: default
batch_size: 32
num_epochs: 20
use_amp: True
clip_grad_norm: 1.0
optimizer: adamw
weight_decay: 0.1
lr: 0.0006
scheduler: linear
lr_step: 20
lr_gamma: 0.5
device: cuda
seed: 42
output_dir: ./Output/GPT2/WIKITEXT103/gelu_s_sigma5.0_s42
test_only: False
total_params: 124439808
trainable_params: 124439808
