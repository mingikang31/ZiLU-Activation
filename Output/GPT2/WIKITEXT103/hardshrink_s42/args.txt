vocab_size: 50257
max_seq_length: 1024
embedding_dim: 768
num_attention_heads: 12
num_layers: 12
dropout: 0.1
activation: hardshrink
sigma: None
inplace: True
dataset: wikitext103
data_path: ./Data
compile: True
compile_mode: default
batch_size: 40
num_epochs: 20
use_amp: True
clip_grad_norm: 1.0
num_workers: 12
persistent_workers: True
prefetch_factor: 3
pin_memory: True
optimizer: adamw
weight_decay: 0.1
lr: 0.0006
scheduler: linear
lr_step: 20
lr_gamma: 0.5
device: cuda
seed: 42
output_dir: ./Output/GPT2/WIKITEXT103/hardshrink_s42
test_only: False
total_params: 124439808
trainable_params: 124439808
