{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09635dac-ac0d-4a82-86db-0ebfcbdcfb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97f4d7a4-c1e3-48f6-b38a-a3afd2184a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CIFAR100, CIFAR10, MNIST Datasets'''\n",
    "import torch \n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import os \n",
    "from pathlib import Path \n",
    "\n",
    "# Huggingface Datasets + GPT Tokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import GPT2Tokenizer \n",
    "\n",
    "\n",
    "'''Wikitext-103 Dataset Class'''\n",
    "class WikiText103:\n",
    "    def __init__(self, args):\n",
    "        self.max_seq_length = args.max_seq_length\n",
    "        self.block_size = self.max_seq_length\n",
    "        self.cache_dir = os.path.join(args.data_path, f\"wikitext103_cache_{args.max_seq_length}\")\n",
    "        \n",
    "        # Tokenizer \n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        if os.path.exists(self.cache_dir):\n",
    "            print(f\"Loading preprocessed dataset from {self.cache_dir}\")\n",
    "            self.lm_dataset = load_from_disk(self.cache_dir)\n",
    "        else: \n",
    "            os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "            # Dataset\n",
    "            self.original_dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "\n",
    "        \n",
    "            self.tokenized_dataset = self.original_dataset.map(\n",
    "                self.tokenize_function, \n",
    "                batched=True, \n",
    "                remove_columns=[\"text\"]\n",
    "            )\n",
    "            \n",
    "            self.lm_dataset = self.tokenized_dataset.map(\n",
    "                self.group_texts, \n",
    "                batched=True\n",
    "            )\n",
    "            self.lm_dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "            self.lm_dataset.save_to_disk(self.cache_dir)\n",
    "            print(f\"Preprocessed dataset saved to {self.cache_dir}\")\n",
    "\n",
    "        # Data Loaders \n",
    "        self.train_loader = DataLoader(dataset=self.lm_dataset[\"train\"], batch_size=args.batch_size, shuffle=True, num_workers=2, pin_memory=False)\n",
    "        self.test_loader = DataLoader(dataset=self.lm_dataset[\"test\"], batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=False)\n",
    "\n",
    "    def group_texts(self, examples): \n",
    "        concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated[list(examples.keys())[0]])\n",
    "        total_length = (total_length // self.block_size) * self.block_size\n",
    "        result = {\n",
    "            k: [t[i : i + self.block_size] for i in range(0, total_length, self.block_size)]\n",
    "            for k, t in concatenated.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "        return self.tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b01077a6-7172-49e2-a2b6-c251d9d919e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.activation import (GELU_s, SiLU_s, ZiLU_Old, ArcTan,\n",
    "                               ArcTan_Approx, ZiLU, ZiLU_Approx)\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, \n",
    "                 args, \n",
    "                 vocab_size=50257, \n",
    "                 max_seq_length=1024,\n",
    "                 embedding_dim=768,\n",
    "                 num_attention_heads=12,\n",
    "                 num_layers=12,\n",
    "                 dropout=0.1,\n",
    "                 device='cuda'):\n",
    "\n",
    "        super(GPT2, self).__init__()\n",
    "\n",
    "        self.args = args \n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        # Dropout \n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        \n",
    "        # Embeddings \n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embedding_dim)     \n",
    "        self.position_embeddings = nn.Embedding(max_seq_length, embedding_dim)\n",
    "        \n",
    "        # Transformer Blocks    \n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(args, embedding_dim, num_attention_heads, embedding_dim * 4, max_seq_length, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final Layer Norm \n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Linear output layer \n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embeddings.weight\n",
    "        \n",
    "        # Initialize Weights \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # Scaled Initialization for Residual Layers \n",
    "        for pn, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                if 'fc2' in pn or 'w_o' in pn:\n",
    "                    p.data.normal_(mean=0.0, std=0.02 / math.sqrt(2 * num_layers))\n",
    "\n",
    "        self.name = f\"GPT2_{num_layers}L_{num_attention_heads}H_{embedding_dim}D_{args.activation}\"\n",
    "        self.to(device)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, input_ids, target=None):\n",
    "        batch_size, seq_length = input_ids.size()\n",
    "\n",
    "        # Create position ids \n",
    "        position_ids = torch.arange(0, seq_length, dtype=torch.long, device=self.device)\n",
    "\n",
    "        # Embeddings \n",
    "        token_embeds = self.token_embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        x = token_embeds + position_embeds\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Transformer Blocks \n",
    "        for layer in self.transformer_blocks:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Final Layer Norm \n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        if target is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None \n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def parameter_count(self, non_embeddings=True): \n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        return total_params, trainable_params\n",
    "    \n",
    "class CausalMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_embeddings, num_heads, max_seq_length, dropout):\n",
    "        super(CausalMultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_embeddings % num_heads == 0, \"Match Embeddings with Number of Heads\"\n",
    "\n",
    "        self.d_embedding = d_embeddings\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.d_heads = d_embeddings // num_heads\n",
    "\n",
    "\n",
    "        self.w_k = nn.Linear(d_embeddings, d_embeddings)\n",
    "        self.w_q = nn.Linear(d_embeddings, d_embeddings)\n",
    "        self.w_v = nn.Linear(d_embeddings, d_embeddings)\n",
    "        self.w_o = nn.Linear(d_embeddings, d_embeddings)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        causal_mask = torch.tril(torch.ones(max_seq_length, max_seq_length)).view(1, 1, max_seq_length, max_seq_length)\n",
    "        self.register_buffer('causal_mask', causal_mask)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_embeddings = x.shape \n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, d_heads = x.shape \n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, num_heads * d_heads)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        k = self.split_heads(self.w_k(x))\n",
    "        q = self.split_heads(self.w_q(x))\n",
    "        v = self.split_heads(self.w_v(x))        \n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * (1.0 / math.sqrt(v.size(-1)))\n",
    "\n",
    "        seq_length = attn_scores.size(-2)\n",
    "        mask_slice = self.causal_mask[:, :, :seq_length, :seq_length]\n",
    "        attn_scores = attn_scores.masked_fill(mask_slice == 0, float('-inf'))\n",
    "\n",
    "        # Softmax and weighting \n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.attn_dropout(attn_probs)\n",
    "        attn_output = torch.matmul(attn_probs, v)\n",
    "\n",
    "        attn_output = self.combine_heads(attn_output)\n",
    "        attn_output = self.w_o(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "        return attn_output\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, args, d_model, d_ff, dropout):\n",
    "        super(MLP, self).__init__() \n",
    "\n",
    "         # Activation Selection\n",
    "        self.activation = args.activation\n",
    "\n",
    "        # Activation function mapping\n",
    "        self.activation_map = {\n",
    "            \"relu\": lambda: nn.ReLU(inplace=args.inplace), \n",
    "            \"silu\": lambda: nn.SiLU(inplace=args.inplace), \n",
    "            \"gelu\": lambda: nn.GELU(), \n",
    "            \"sigmoid\": lambda: nn.Sigmoid(), \n",
    "\n",
    "            # Previous Activation Generation\n",
    "            \"gelu_s\": lambda: GELU_s(sigma=args.sigma, inplace=args.inplace), \n",
    "            \"silu_s\": lambda: SiLU_s(sigma=args.sigma, inplace=args.inplace), \n",
    "            \"zilu_old\": lambda: ZiLU_Old(sigma=args.sigma, inplace=args.inplace), \n",
    "\n",
    "            # Current Activation Generation \n",
    "            \"arctan\": lambda: ArcTan(sigma=args.sigma), \n",
    "            \"arctan_approx\": lambda: ArcTan_Approx(sigma=args.sigma), \n",
    "            \"zilu\": lambda: ZiLU(sigma=args.sigma), \n",
    "            \"zilu_approx\": lambda: ZiLU_Approx(sigma=args.sigma) \n",
    "        }\n",
    "\n",
    "        if self.activation not in self.activation_map:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation}\")\n",
    "            \n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.activation_function = self.activation_map[self.activation]()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_function(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, args, d_model, num_heads, d_ff, max_seq_length, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = CausalMultiHeadAttention(d_model, num_heads, max_seq_length, dropout)\n",
    "        self.mlp = MLP(args, d_model, d_ff, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-Norm Multi-Head Attention \n",
    "        norm_x = self.layer_norm1(x)\n",
    "        attn_output = self.attention(norm_x)\n",
    "        x = x + attn_output\n",
    "\n",
    "        # Post-Norm MLP\n",
    "        norm_x = self.layer_norm2(x)\n",
    "        mlp_output = self.mlp(norm_x)\n",
    "        x = x + mlp_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb147760-e214-499f-9102-5e73e81b8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import math    \n",
    "args = SimpleNamespace(\n",
    "        vocab_size=50257,\n",
    "        max_seq_length=1024, \n",
    "        embedding_dim=768, \n",
    "        num_attention_heads=12, \n",
    "        num_layers=12, \n",
    "        dropout=0.1, \n",
    "        activation='gelu', \n",
    "        sigma=1.0, \n",
    "        dataset='wikitext103', \n",
    "        data_path='./Data', \n",
    "        device='cuda',\n",
    "        batch_size=8,\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5b9bb47-8b79-46aa-b080-61602f69a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2(\n",
    "        args=args,\n",
    "        vocab_size=args.vocab_size,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "        embedding_dim=args.embedding_dim,\n",
    "        num_attention_heads=args.num_attention_heads,\n",
    "        num_layers=args.num_layers,\n",
    "        dropout=args.dropout,\n",
    "        device=args.device\n",
    "    )\n",
    "\n",
    "device = args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "317df25c-863b-4b9f-ad60-83f88a7d2d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed dataset from ./Data/wikitext103_cache_1024\n"
     ]
    }
   ],
   "source": [
    "dataset = WikiText103(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d075647-59c1-4495-8c79-d123c47cdfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024])\n",
      "torch.Size([8, 1023])\n",
      "torch.Size([8, 1023])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataset.train_loader))\n",
    "tokens = batch[\"input_ids\"]\n",
    "\n",
    "inputs = tokens[:, :-1].contiguous()\n",
    "targets = tokens[:, 1:].contiguous()\n",
    "\n",
    "print(tokens.shape)\n",
    "print(inputs.shape)\n",
    "print(targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8308b590-d4c8-4589-a304-6550f4845e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs (last 10):  [764, 1355, 7637, 837, 508, 550, 587, 6623, 287, 10173]\n",
      "Target IDs (last 10): [1355, 7637, 837, 508, 550, 587, 6623, 287, 10173, 312]\n",
      "----------------------------------------\n",
      "Input Text (last 5 words):    had been resident in Arc\n",
      "Target Text (last 5 words):   been resident in Arcid\n"
     ]
    }
   ],
   "source": [
    "# Get the tokenizer to decode the numbers back to words\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Take the first sequence in the batch\n",
    "inp_ids = inputs[0]\n",
    "tgt_ids = targets[0]\n",
    "\n",
    "# Print the last 10 tokens as numbers\n",
    "print(\"Input IDs (last 10): \", inp_ids[-10:].tolist())\n",
    "print(\"Target IDs (last 10):\", tgt_ids[-10:].tolist())\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Decode to text to see the shift visually\n",
    "print(\"Input Text (last 5 words):  \", tokenizer.decode(inp_ids[-5:]))\n",
    "print(\"Target Text (last 5 words): \", tokenizer.decode(tgt_ids[-5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a299f4-cf44-4c9f-9bab-7e11c34b7b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
