{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09035df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Mounting onto Google Colab\n",
    "from google.colab import files \n",
    "files.download('examples.txt')\n",
    "\n",
    "from google.colab import drive \n",
    "drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e79f9a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "GELU with adjustable parameter a\n",
    "SiLU with adjustable parameter a\n",
    "ZiLU with adjustable parameter s\n",
    "\"\"\"\n",
    "class GELU_s(nn.Module):\n",
    "    def __init__(self, sigma, inplace=False, max_val=1000):\n",
    "        super(GELU_s, self).__init__()\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.max_val = max_val\n",
    "        self.kAlpha = 0.70710678118654752440\n",
    "        self.relu = nn.ReLU(inplace=inplace) \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.sigma >= self.max_val:\n",
    "            return self.relu(x) \n",
    "        else: \n",
    "            return x * 0.5 * (1 + torch.erf(self.sigma * x * self.kAlpha))\n",
    "\n",
    "class SiLU_s(nn.Module):\n",
    "    def __init__(self, sigma, inplace=False, max_val=1000):\n",
    "        super(SiLU_s, self).__init__()\n",
    "        self.sigma = sigma\n",
    "        self.max_val = max_val\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.sigma >= self.max_val:\n",
    "            return self.relu(x)\n",
    "        else:\n",
    "            return x * torch.sigmoid(self.sigma * x)\n",
    "\n",
    "class ZiLU_Old(nn.Module):\n",
    "    def __init__(self, sigma, inplace=False, max_val=1000):\n",
    "        super(ZiLU_Old, self).__init__()\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.max_val = max_val\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.sigma >= self.max_val:\n",
    "            return self.relu(x)\n",
    "        else:\n",
    "            return x * (2 * (1/4 + 1/(2 * torch.pi) * torch.arctan(self.sigma * x)))\n",
    "\n",
    "\"\"\"\n",
    "arctan \n",
    "arctan approximation \n",
    "ZiLU\n",
    "ZiLU approximation\n",
    "\"\"\"\n",
    "\n",
    "class ArcTan(nn.Module):\n",
    "    def __init__(self, sigma=None):\n",
    "        super(ArcTan, self).__init__()\n",
    "        if sigma: \n",
    "            self.sigma = sigma\n",
    "        else: \n",
    "            self.sigma = nn.Parameter(torch.tensor(5.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 + (1.0 / torch.pi) * torch.arctan(self.sigma * x)\n",
    "\n",
    "class ArcTan_Approx(nn.Module):\n",
    "    def __init__(self, sigma=None):\n",
    "        super(ArcTan_Approx, self).__init__()\n",
    "        if sigma: \n",
    "            self.sigma = sigma\n",
    "        else: \n",
    "            self.sigma = nn.Parameter(torch.tensor(5.0))\n",
    "\n",
    "    def forward(self, x): \n",
    "        z = self.sigma * x \n",
    "        return (0.5 + torch.clamp(z, min=0)) / (1.0 + torch.abs(z))\n",
    "\n",
    "class ZiLU(nn.Module):\n",
    "    def __init__(self, sigma=None):\n",
    "        super(ZiLU, self).__init__()\n",
    "        self.arctan = ArcTan(sigma)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.arctan(x)\n",
    "\n",
    "class ZiLU_Approx(nn.Module):\n",
    "    def __init__(self, sigma=None):\n",
    "        super(ZiLU_Approx, self).__init__()\n",
    "        self.arctan_approx = ArcTan_Approx(sigma)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.arctan_approx(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bb7cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: ['cpu', 'mps']\n",
      "Input shape: torch.Size([64, 1024])\n",
      "\n",
      "\n",
      "============================================================\n",
      "Device: CPU\n",
      "============================================================\n",
      "\n",
      "Benchmarking ReLU...\n",
      "  Forward:  0.0549 ± 0.0182 ms\n",
      "  Backward: 0.0804 ± 0.0415 ms\n",
      "\n",
      "Benchmarking SiLU...\n",
      "  Forward:  0.0509 ± 0.0490 ms\n",
      "  Backward: 0.0918 ± 0.0156 ms\n",
      "\n",
      "Benchmarking GELU...\n",
      "  Forward:  0.0647 ± 0.0416 ms\n",
      "  Backward: 0.1205 ± 0.0118 ms\n",
      "\n",
      "Benchmarking Sigmoid...\n",
      "  Forward:  0.0532 ± 0.0330 ms\n",
      "  Backward: 0.0828 ± 0.0074 ms\n",
      "\n",
      "Benchmarking LeakyReLU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1228 14:28:40.910000 24020 site-packages/torch/_dynamo/convert_frame.py:1016] [0/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W1228 14:28:40.910000 24020 site-packages/torch/_dynamo/convert_frame.py:1016] [0/8]    function: 'inner' (/Users/mingikang/miniconda3/envs/torch/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)\n",
      "W1228 14:28:40.910000 24020 site-packages/torch/_dynamo/convert_frame.py:1016] [0/8]    last reason: 0/7: ___check_type_id(fn, 4408782496)                       \n",
      "W1228 14:28:40.910000 24020 site-packages/torch/_dynamo/convert_frame.py:1016] [0/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1228 14:28:40.910000 24020 site-packages/torch/_dynamo/convert_frame.py:1016] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Forward:  0.0700 ± 0.0505 ms\n",
      "  Backward: 0.0947 ± 0.0428 ms\n",
      "\n",
      "Benchmarking PReLU...\n",
      "  Forward:  0.0406 ± 0.0370 ms\n",
      "  Backward: 0.0833 ± 0.0109 ms\n",
      "\n",
      "Benchmarking ELU...\n",
      "  Forward:  0.0472 ± 0.0346 ms\n",
      "  Backward: 0.0939 ± 0.0113 ms\n",
      "\n",
      "Benchmarking Hardshrink...\n",
      "  Forward:  0.0562 ± 0.0386 ms\n",
      "  Backward: 0.0850 ± 0.0278 ms\n",
      "\n",
      "Benchmarking Softshrink...\n",
      "  Forward:  0.0240 ± 0.0145 ms\n",
      "  Backward: 0.0327 ± 0.0036 ms\n",
      "\n",
      "Benchmarking Tanhshrink...\n",
      "  Forward:  0.0878 ± 0.0376 ms\n",
      "  Backward: 0.0868 ± 0.0350 ms\n",
      "\n",
      "Benchmarking Hardtanh...\n",
      "  Forward:  0.0297 ± 0.0447 ms\n",
      "  Backward: 0.0322 ± 0.0049 ms\n",
      "\n",
      "Benchmarking Softplus...\n",
      "  Forward:  0.1119 ± 0.0338 ms\n",
      "  Backward: 0.0726 ± 0.0052 ms\n",
      "\n",
      "Benchmarking Softsign...\n",
      "  Forward:  0.0696 ± 0.0447 ms\n",
      "  Backward: 0.1669 ± 0.0452 ms\n",
      "\n",
      "Benchmarking Tanh...\n",
      "  Forward:  0.0646 ± 0.0198 ms\n",
      "  Backward: 0.0421 ± 0.0037 ms\n",
      "\n",
      "Benchmarking CELU...\n",
      "  Forward:  0.0888 ± 0.0041 ms\n",
      "  Backward: 0.0721 ± 0.0046 ms\n",
      "\n",
      "Benchmarking Swish...\n",
      "  Forward:  0.0624 ± 0.0217 ms\n",
      "  Backward: 0.0995 ± 0.0155 ms\n",
      "\n",
      "Benchmarking Mish...\n",
      "  Forward:  0.2421 ± 0.0376 ms\n",
      "  Backward: 0.2871 ± 0.0178 ms\n",
      "\n",
      "Benchmarking HardSwish...\n",
      "  Forward:  0.0209 ± 0.0036 ms\n",
      "  Backward: 0.0436 ± 0.0085 ms\n",
      "\n",
      "Benchmarking HardSigmoid...\n",
      "  Forward:  0.0246 ± 0.0312 ms\n",
      "  Backward: 0.0308 ± 0.0028 ms\n",
      "\n",
      "Benchmarking GELU_s...\n",
      "  Forward:  0.0582 ± 0.0298 ms\n",
      "  Backward: 0.1223 ± 0.0156 ms\n",
      "\n",
      "Benchmarking SiLU_s...\n",
      "  Forward:  0.0500 ± 0.0312 ms\n",
      "  Backward: 0.0973 ± 0.0063 ms\n",
      "\n",
      "Benchmarking ZiLU_Old...\n",
      "  Forward:  0.0647 ± 0.0188 ms\n",
      "  Backward: 0.1205 ± 0.0108 ms\n",
      "\n",
      "Benchmarking ArcTan...\n",
      "  Forward:  0.0659 ± 0.0389 ms\n",
      "  Backward: 0.0852 ± 0.0075 ms\n",
      "\n",
      "Benchmarking ArcTan_Approx...\n",
      "  Forward:  0.0341 ± 0.0199 ms\n",
      "  Backward: 0.0844 ± 0.0123 ms\n",
      "\n",
      "Benchmarking ZiLU...\n",
      "  Forward:  0.0810 ± 0.0430 ms\n",
      "  Backward: 0.1329 ± 0.0340 ms\n",
      "\n",
      "Benchmarking ZiLU_Approx...\n",
      "  Forward:  0.0366 ± 0.0280 ms\n",
      "  Backward: 0.1064 ± 0.0382 ms\n",
      "\n",
      "============================================================\n",
      "Device: MPS\n",
      "============================================================\n",
      "\n",
      "Benchmarking ReLU...\n",
      "  Forward:  0.3296 ± 0.3062 ms\n",
      "  Backward: 0.4836 ± 0.3152 ms\n",
      "\n",
      "Benchmarking SiLU...\n",
      "  Forward:  0.3169 ± 0.2644 ms\n",
      "  Backward: 0.5140 ± 0.3316 ms\n",
      "\n",
      "Benchmarking GELU...\n",
      "  Forward:  0.2095 ± 0.0246 ms\n",
      "  Backward: 0.3372 ± 0.0379 ms\n",
      "\n",
      "Benchmarking Sigmoid...\n",
      "  Forward:  0.2188 ± 0.0675 ms\n",
      "  Backward: 0.3204 ± 0.0329 ms\n",
      "\n",
      "Benchmarking LeakyReLU...\n",
      "  Forward:  0.1835 ± 0.0256 ms\n",
      "  Backward: 0.3289 ± 0.0452 ms\n",
      "\n",
      "Benchmarking PReLU...\n",
      "  Forward:  0.2274 ± 0.0411 ms\n",
      "  Backward: 0.3547 ± 0.0546 ms\n",
      "\n",
      "Benchmarking ELU...\n",
      "  Forward:  0.1866 ± 0.0419 ms\n",
      "  Backward: 0.3228 ± 0.0327 ms\n",
      "\n",
      "Benchmarking Hardshrink...\n",
      "  Forward:  0.1900 ± 0.0297 ms\n",
      "  Backward: 0.3237 ± 0.0517 ms\n",
      "\n",
      "Benchmarking Softshrink...\n",
      "  Forward:  0.1864 ± 0.0350 ms\n",
      "  Backward: 0.3270 ± 0.0477 ms\n",
      "\n",
      "Benchmarking Tanhshrink...\n",
      "  Forward:  0.1937 ± 0.0228 ms\n",
      "  Backward: 0.3649 ± 0.0298 ms\n",
      "\n",
      "Benchmarking Hardtanh...\n",
      "  Forward:  0.2076 ± 0.0262 ms\n",
      "  Backward: 0.3266 ± 0.0301 ms\n",
      "\n",
      "Benchmarking Softplus...\n",
      "  Forward:  0.1981 ± 0.0291 ms\n",
      "  Backward: 0.3149 ± 0.0280 ms\n",
      "\n",
      "Benchmarking Softsign...\n",
      "  Forward:  0.2449 ± 0.0338 ms\n",
      "  Backward: 0.3942 ± 0.0364 ms\n",
      "\n",
      "Benchmarking Tanh...\n",
      "  Forward:  0.1854 ± 0.0265 ms\n",
      "  Backward: 0.5426 ± 0.5516 ms\n",
      "\n",
      "Benchmarking CELU...\n",
      "  Forward:  0.5249 ± 0.7412 ms\n",
      "  Backward: 0.7042 ± 0.8839 ms\n",
      "\n",
      "Benchmarking Swish...\n",
      "  Forward:  0.2744 ± 0.0331 ms\n",
      "  Backward: 0.4351 ± 0.0991 ms\n",
      "\n",
      "Benchmarking Mish...\n",
      "  Forward:  0.2139 ± 0.0351 ms\n",
      "  Backward: 0.3429 ± 0.0385 ms\n",
      "\n",
      "Benchmarking HardSwish...\n",
      "  Forward:  0.1829 ± 0.0225 ms\n",
      "  Backward: 0.3335 ± 0.0385 ms\n",
      "\n",
      "Benchmarking HardSigmoid...\n",
      "  Forward:  0.1884 ± 0.0256 ms\n",
      "  Backward: 0.3229 ± 0.0461 ms\n",
      "\n",
      "Benchmarking GELU_s...\n",
      "  Forward:  0.1971 ± 0.0552 ms\n",
      "  Backward: 0.3646 ± 0.0419 ms\n",
      "\n",
      "Benchmarking SiLU_s...\n",
      "  Forward:  0.2212 ± 0.0460 ms\n",
      "  Backward: 0.3596 ± 0.0322 ms\n",
      "\n",
      "Benchmarking ZiLU_Old...\n",
      "  Forward:  0.1903 ± 0.0414 ms\n",
      "  Backward: 0.3535 ± 0.0266 ms\n",
      "\n",
      "Benchmarking ArcTan...\n",
      "  Forward:  0.2784 ± 0.4513 ms\n",
      "  Backward: 0.3878 ± 0.0750 ms\n",
      "\n",
      "Benchmarking ArcTan_Approx...\n",
      "  Forward:  0.2043 ± 0.0302 ms\n",
      "  Backward: 0.3536 ± 0.0471 ms\n",
      "\n",
      "Benchmarking ZiLU...\n",
      "  Forward:  0.1933 ± 0.0230 ms\n",
      "  Backward: 0.3527 ± 0.0318 ms\n",
      "\n",
      "Benchmarking ZiLU_Approx...\n",
      "  Forward:  0.1979 ± 0.0181 ms\n",
      "  Backward: 0.3602 ± 0.0383 ms\n",
      "\n",
      "============================================================\n",
      "SUMMARY TABLE\n",
      "============================================================\n",
      "\n",
      "CPU:\n",
      "Activation           Forward (ms)         Backward (ms)       \n",
      "------------------------------------------------------------\n",
      "ReLU                 0.0549 ± 0.0182      0.0804 ± 0.0415     \n",
      "SiLU                 0.0509 ± 0.0490      0.0918 ± 0.0156     \n",
      "GELU                 0.0647 ± 0.0416      0.1205 ± 0.0118     \n",
      "Sigmoid              0.0532 ± 0.0330      0.0828 ± 0.0074     \n",
      "LeakyReLU            0.0700 ± 0.0505      0.0947 ± 0.0428     \n",
      "PReLU                0.0406 ± 0.0370      0.0833 ± 0.0109     \n",
      "ELU                  0.0472 ± 0.0346      0.0939 ± 0.0113     \n",
      "Hardshrink           0.0562 ± 0.0386      0.0850 ± 0.0278     \n",
      "Softshrink           0.0240 ± 0.0145      0.0327 ± 0.0036     \n",
      "Tanhshrink           0.0878 ± 0.0376      0.0868 ± 0.0350     \n",
      "Hardtanh             0.0297 ± 0.0447      0.0322 ± 0.0049     \n",
      "Softplus             0.1119 ± 0.0338      0.0726 ± 0.0052     \n",
      "Softsign             0.0696 ± 0.0447      0.1669 ± 0.0452     \n",
      "Tanh                 0.0646 ± 0.0198      0.0421 ± 0.0037     \n",
      "CELU                 0.0888 ± 0.0041      0.0721 ± 0.0046     \n",
      "Swish                0.0624 ± 0.0217      0.0995 ± 0.0155     \n",
      "Mish                 0.2421 ± 0.0376      0.2871 ± 0.0178     \n",
      "HardSwish            0.0209 ± 0.0036      0.0436 ± 0.0085     \n",
      "HardSigmoid          0.0246 ± 0.0312      0.0308 ± 0.0028     \n",
      "GELU_s               0.0582 ± 0.0298      0.1223 ± 0.0156     \n",
      "SiLU_s               0.0500 ± 0.0312      0.0973 ± 0.0063     \n",
      "ZiLU_Old             0.0647 ± 0.0188      0.1205 ± 0.0108     \n",
      "ArcTan               0.0659 ± 0.0389      0.0852 ± 0.0075     \n",
      "ArcTan_Approx        0.0341 ± 0.0199      0.0844 ± 0.0123     \n",
      "ZiLU                 0.0810 ± 0.0430      0.1329 ± 0.0340     \n",
      "ZiLU_Approx          0.0366 ± 0.0280      0.1064 ± 0.0382     \n",
      "\n",
      "MPS:\n",
      "Activation           Forward (ms)         Backward (ms)       \n",
      "------------------------------------------------------------\n",
      "ReLU                 0.3296 ± 0.3062      0.4836 ± 0.3152     \n",
      "SiLU                 0.3169 ± 0.2644      0.5140 ± 0.3316     \n",
      "GELU                 0.2095 ± 0.0246      0.3372 ± 0.0379     \n",
      "Sigmoid              0.2188 ± 0.0675      0.3204 ± 0.0329     \n",
      "LeakyReLU            0.1835 ± 0.0256      0.3289 ± 0.0452     \n",
      "PReLU                0.2274 ± 0.0411      0.3547 ± 0.0546     \n",
      "ELU                  0.1866 ± 0.0419      0.3228 ± 0.0327     \n",
      "Hardshrink           0.1900 ± 0.0297      0.3237 ± 0.0517     \n",
      "Softshrink           0.1864 ± 0.0350      0.3270 ± 0.0477     \n",
      "Tanhshrink           0.1937 ± 0.0228      0.3649 ± 0.0298     \n",
      "Hardtanh             0.2076 ± 0.0262      0.3266 ± 0.0301     \n",
      "Softplus             0.1981 ± 0.0291      0.3149 ± 0.0280     \n",
      "Softsign             0.2449 ± 0.0338      0.3942 ± 0.0364     \n",
      "Tanh                 0.1854 ± 0.0265      0.5426 ± 0.5516     \n",
      "CELU                 0.5249 ± 0.7412      0.7042 ± 0.8839     \n",
      "Swish                0.2744 ± 0.0331      0.4351 ± 0.0991     \n",
      "Mish                 0.2139 ± 0.0351      0.3429 ± 0.0385     \n",
      "HardSwish            0.1829 ± 0.0225      0.3335 ± 0.0385     \n",
      "HardSigmoid          0.1884 ± 0.0256      0.3229 ± 0.0461     \n",
      "GELU_s               0.1971 ± 0.0552      0.3646 ± 0.0419     \n",
      "SiLU_s               0.2212 ± 0.0460      0.3596 ± 0.0322     \n",
      "ZiLU_Old             0.1903 ± 0.0414      0.3535 ± 0.0266     \n",
      "ArcTan               0.2784 ± 0.4513      0.3878 ± 0.0750     \n",
      "ArcTan_Approx        0.2043 ± 0.0302      0.3536 ± 0.0471     \n",
      "ZiLU                 0.1933 ± 0.0230      0.3527 ± 0.0318     \n",
      "ZiLU_Approx          0.1979 ± 0.0181      0.3602 ± 0.0383     \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def benchmark_activation(activation_fn, input_tensor, device, num_warmup=10, num_iterations=100, compile=True):\n",
    "    \"\"\"\n",
    "    Benchmark forward and backward pass times for an activation function.\n",
    "    \n",
    "    Args:\n",
    "        activation_fn: The activation function module\n",
    "        input_tensor: Input tensor for testing\n",
    "        device: Device to run on ('cpu', 'mps', 'cuda')\n",
    "        num_warmup: Number of warmup iterations\n",
    "        num_iterations: Number of timed iterations\n",
    "    \n",
    "    Returns:\n",
    "        dict with forward_time and backward_time in milliseconds\n",
    "    \"\"\"\n",
    "    activation_fn = activation_fn.to(device)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    if compile: \n",
    "        activation_fn = torch.compile(activation_fn)\n",
    "        \n",
    "    # Warmup\n",
    "    for _ in range(num_warmup):\n",
    "        output = activation_fn(input_tensor)\n",
    "        if input_tensor.requires_grad:\n",
    "            output.sum().backward()\n",
    "            input_tensor.grad = None\n",
    "    \n",
    "    # Synchronize device\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    elif device == 'mps':\n",
    "        torch.mps.synchronize()\n",
    "    \n",
    "    # Benchmark forward pass\n",
    "    forward_times = []\n",
    "    for _ in range(num_iterations):\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "        elif device == 'mps':\n",
    "            torch.mps.synchronize()\n",
    "            start = time.perf_counter()\n",
    "        else:\n",
    "            start = time.perf_counter()\n",
    "        \n",
    "        output = activation_fn(input_tensor)\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        elif device == 'mps':\n",
    "            torch.mps.synchronize()\n",
    "        \n",
    "        end = time.perf_counter()\n",
    "        forward_times.append((end - start) * 1000)  # Convert to ms\n",
    "    \n",
    "    # Benchmark backward pass\n",
    "    backward_times = []\n",
    "    for _ in range(num_iterations):\n",
    "        output = activation_fn(input_tensor)\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "        elif device == 'mps':\n",
    "            torch.mps.synchronize()\n",
    "            start = time.perf_counter()\n",
    "        else:\n",
    "            start = time.perf_counter()\n",
    "        \n",
    "        output.sum().backward()\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        elif device == 'mps':\n",
    "            torch.mps.synchronize()\n",
    "        \n",
    "        end = time.perf_counter()\n",
    "        backward_times.append((end - start) * 1000)  # Convert to ms\n",
    "        \n",
    "        input_tensor.grad = None\n",
    "    \n",
    "    return {\n",
    "        'forward_mean': sum(forward_times) / len(forward_times),\n",
    "        'forward_std': torch.tensor(forward_times).std().item(),\n",
    "        'backward_mean': sum(backward_times) / len(backward_times),\n",
    "        'backward_std': torch.tensor(backward_times).std().item()\n",
    "    }\n",
    "\n",
    "def run_benchmarks():\n",
    "    \"\"\"Run benchmarks for all activation functions on all available devices.\"\"\"\n",
    "    \n",
    "    # Test configuration\n",
    "    batch_size = 64\n",
    "    input_size = 1024\n",
    "    sigma = 5.0\n",
    "    \n",
    "    # Create test input\n",
    "    input_tensor = torch.randn(batch_size, input_size, requires_grad=True)\n",
    "    \n",
    "    # Activation functions to test\n",
    "    activations = {\n",
    "        \"ReLU\": nn.ReLU(),\n",
    "        \"SiLU\": nn.SiLU(),\n",
    "        \"GELU\": nn.GELU(),\n",
    "        \"Sigmoid\": nn.Sigmoid(),\n",
    "        \"LeakyReLU\": nn.LeakyReLU(),\n",
    "        \"PReLU\": nn.PReLU(),\n",
    "        \"ELU\": nn.ELU(),\n",
    "        \"Hardshrink\": nn.Hardshrink(),\n",
    "        \"Softshrink\": nn.Softshrink(),\n",
    "        \"Tanhshrink\": nn.Tanhshrink(),\n",
    "        \"Hardtanh\": nn.Hardtanh(),\n",
    "        \"Softplus\": nn.Softplus(),\n",
    "        \"Softsign\": nn.Softsign(),\n",
    "        \"Tanh\": nn.Tanh(),\n",
    "        \"CELU\": nn.CELU(),\n",
    "        \"Swish\": nn.SiLU(),  # Swish is equivalent to SiLU\n",
    "        \"Mish\": nn.Mish(),\n",
    "        \"HardSwish\": nn.Hardswish(),\n",
    "        \"HardSigmoid\": nn.Hardsigmoid(),\n",
    "        \"GELU_s\": GELU_s(sigma=sigma),\n",
    "        \"SiLU_s\": SiLU_s(sigma=sigma),\n",
    "        \"ZiLU_Old\": ZiLU_Old(sigma=sigma),\n",
    "        \"ArcTan\": ArcTan(sigma=sigma),\n",
    "        \"ArcTan_Approx\": ArcTan_Approx(sigma=sigma),\n",
    "        \"ZiLU\": ZiLU(sigma=sigma),\n",
    "        \"ZiLU_Approx\": ZiLU_Approx(sigma=sigma)\n",
    "    }\n",
    "    \n",
    "    # Determine available devices\n",
    "    devices = ['cpu']\n",
    "    if torch.cuda.is_available():\n",
    "        devices.append('cuda')\n",
    "    if torch.backends.mps.is_available():\n",
    "        devices.append('mps')\n",
    "    \n",
    "    print(f\"Available devices: {devices}\")\n",
    "    print(f\"Input shape: {input_tensor.shape}\\n\")\n",
    "    \n",
    "    # Run benchmarks\n",
    "    results = {}\n",
    "    for device in devices:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Device: {device.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        results[device] = {}\n",
    "        \n",
    "        for name, activation_fn in activations.items():\n",
    "            print(f\"\\nBenchmarking {name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Create fresh input for each test\n",
    "                test_input = input_tensor.clone().detach().requires_grad_(True)\n",
    "                \n",
    "                # Run benchmark\n",
    "                timing = benchmark_activation(\n",
    "                    activation_fn, \n",
    "                    test_input, \n",
    "                    device,\n",
    "                    num_warmup=10,\n",
    "                    num_iterations=100\n",
    "                )\n",
    "                \n",
    "                results[device][name] = timing\n",
    "                \n",
    "                print(f\"  Forward:  {timing['forward_mean']:.4f} ± {timing['forward_std']:.4f} ms\")\n",
    "                print(f\"  Backward: {timing['backward_mean']:.4f} ± {timing['backward_std']:.4f} ms\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "                results[device][name] = None\n",
    "    \n",
    "    # Print summary table\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for device in devices:\n",
    "        print(f\"\\n{device.upper()}:\")\n",
    "        print(f\"{'Activation':<20} {'Forward (ms)':<20} {'Backward (ms)':<20}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for name in activations.keys():\n",
    "            if results[device].get(name):\n",
    "                timing = results[device][name]\n",
    "                fwd = f\"{timing['forward_mean']:.4f} ± {timing['forward_std']:.4f}\"\n",
    "                bwd = f\"{timing['backward_mean']:.4f} ± {timing['backward_std']:.4f}\"\n",
    "                print(f\"{name:<20} {fwd:<20} {bwd:<20}\")\n",
    "            else:\n",
    "                print(f\"{name:<20} {'N/A':<20} {'N/A':<20}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae7e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def results_to_dataframe(results):\n",
    "    \"\"\"\n",
    "    Convert benchmark results to a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        results: Nested dict with structure {device: {activation: {metric: value}}}\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with columns: Device, Activation, Forward_Mean, Forward_Std, Backward_Mean, Backward_Std\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for device, activations in results.items():\n",
    "        for activation_name, timing in activations.items():\n",
    "            if timing is not None:\n",
    "                data.append({\n",
    "                    'Device': device,\n",
    "                    'Activation': activation_name,\n",
    "                    'Forward_Mean (ms)': timing['forward_mean'],\n",
    "                    'Forward_Std (ms)': timing['forward_std'],\n",
    "                    'Backward_Mean (ms)': timing['backward_mean'],\n",
    "                    'Backward_Std (ms)': timing['backward_std']\n",
    "                })\n",
    "            else:\n",
    "                data.append({\n",
    "                    'Device': device,\n",
    "                    'Activation': activation_name,\n",
    "                    'Forward_Mean (ms)': None,\n",
    "                    'Forward_Std (ms)': None,\n",
    "                    'Backward_Mean (ms)': None,\n",
    "                    'Backward_Std (ms)': None\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad66568d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Device     Activation  Forward_Mean (ms)  Forward_Std (ms)  \\\n",
      "0     cpu           ReLU           0.054890          0.018215   \n",
      "1     cpu           SiLU           0.050897          0.048995   \n",
      "2     cpu           GELU           0.064667          0.041575   \n",
      "3     cpu        Sigmoid           0.053166          0.033032   \n",
      "4     cpu      LeakyReLU           0.070024          0.050513   \n",
      "5     cpu          PReLU           0.040607          0.036993   \n",
      "6     cpu            ELU           0.047236          0.034580   \n",
      "7     cpu     Hardshrink           0.056221          0.038612   \n",
      "8     cpu     Softshrink           0.024031          0.014457   \n",
      "9     cpu     Tanhshrink           0.087765          0.037627   \n",
      "10    cpu       Hardtanh           0.029659          0.044713   \n",
      "11    cpu       Softplus           0.111935          0.033768   \n",
      "12    cpu       Softsign           0.069600          0.044728   \n",
      "13    cpu           Tanh           0.064608          0.019848   \n",
      "14    cpu           CELU           0.088837          0.004101   \n",
      "15    cpu          Swish           0.062388          0.021698   \n",
      "16    cpu           Mish           0.242125          0.037562   \n",
      "17    cpu      HardSwish           0.020925          0.003607   \n",
      "18    cpu    HardSigmoid           0.024562          0.031231   \n",
      "19    cpu         GELU_s           0.058173          0.029832   \n",
      "20    cpu         SiLU_s           0.049992          0.031232   \n",
      "21    cpu       ZiLU_Old           0.064707          0.018845   \n",
      "22    cpu         ArcTan           0.065949          0.038883   \n",
      "23    cpu  ArcTan_Approx           0.034138          0.019906   \n",
      "24    cpu           ZiLU           0.081046          0.042967   \n",
      "25    cpu    ZiLU_Approx           0.036644          0.028018   \n",
      "26    mps           ReLU           0.329644          0.306221   \n",
      "27    mps           SiLU           0.316856          0.264403   \n",
      "28    mps           GELU           0.209480          0.024610   \n",
      "29    mps        Sigmoid           0.218775          0.067500   \n",
      "30    mps      LeakyReLU           0.183487          0.025559   \n",
      "31    mps          PReLU           0.227438          0.041066   \n",
      "32    mps            ELU           0.186566          0.041931   \n",
      "33    mps     Hardshrink           0.190036          0.029664   \n",
      "34    mps     Softshrink           0.186384          0.035047   \n",
      "35    mps     Tanhshrink           0.193678          0.022794   \n",
      "36    mps       Hardtanh           0.207591          0.026198   \n",
      "37    mps       Softplus           0.198065          0.029110   \n",
      "38    mps       Softsign           0.244890          0.033835   \n",
      "39    mps           Tanh           0.185370          0.026542   \n",
      "40    mps           CELU           0.524935          0.741193   \n",
      "41    mps          Swish           0.274433          0.033124   \n",
      "42    mps           Mish           0.213919          0.035059   \n",
      "43    mps      HardSwish           0.182869          0.022451   \n",
      "44    mps    HardSigmoid           0.188357          0.025628   \n",
      "45    mps         GELU_s           0.197149          0.055195   \n",
      "46    mps         SiLU_s           0.221249          0.045999   \n",
      "47    mps       ZiLU_Old           0.190318          0.041413   \n",
      "48    mps         ArcTan           0.278443          0.451286   \n",
      "49    mps  ArcTan_Approx           0.204268          0.030203   \n",
      "50    mps           ZiLU           0.193329          0.022969   \n",
      "51    mps    ZiLU_Approx           0.197913          0.018067   \n",
      "\n",
      "    Backward_Mean (ms)  Backward_Std (ms)  \n",
      "0             0.080372           0.041508  \n",
      "1             0.091777           0.015623  \n",
      "2             0.120515           0.011821  \n",
      "3             0.082849           0.007444  \n",
      "4             0.094693           0.042790  \n",
      "5             0.083297           0.010853  \n",
      "6             0.093852           0.011269  \n",
      "7             0.085047           0.027789  \n",
      "8             0.032727           0.003641  \n",
      "9             0.086763           0.034957  \n",
      "10            0.032187           0.004940  \n",
      "11            0.072584           0.005215  \n",
      "12            0.166872           0.045152  \n",
      "13            0.042055           0.003727  \n",
      "14            0.072115           0.004620  \n",
      "15            0.099515           0.015518  \n",
      "16            0.287096           0.017815  \n",
      "17            0.043601           0.008535  \n",
      "18            0.030840           0.002827  \n",
      "19            0.122331           0.015639  \n",
      "20            0.097318           0.006332  \n",
      "21            0.120547           0.010848  \n",
      "22            0.085219           0.007538  \n",
      "23            0.084357           0.012303  \n",
      "24            0.132888           0.034000  \n",
      "25            0.106395           0.038194  \n",
      "26            0.483629           0.315240  \n",
      "27            0.513987           0.331598  \n",
      "28            0.337236           0.037871  \n",
      "29            0.320386           0.032908  \n",
      "30            0.328928           0.045227  \n",
      "31            0.354722           0.054645  \n",
      "32            0.322845           0.032694  \n",
      "33            0.323742           0.051671  \n",
      "34            0.326954           0.047736  \n",
      "35            0.364871           0.029802  \n",
      "36            0.326633           0.030117  \n",
      "37            0.314924           0.027960  \n",
      "38            0.394173           0.036447  \n",
      "39            0.542640           0.551646  \n",
      "40            0.704189           0.883949  \n",
      "41            0.435069           0.099130  \n",
      "42            0.342893           0.038541  \n",
      "43            0.333464           0.038506  \n",
      "44            0.322854           0.046132  \n",
      "45            0.364623           0.041885  \n",
      "46            0.359594           0.032229  \n",
      "47            0.353499           0.026553  \n",
      "48            0.387831           0.074967  \n",
      "49            0.353634           0.047081  \n",
      "50            0.352747           0.031825  \n",
      "51            0.360158           0.038279  \n"
     ]
    }
   ],
   "source": [
    "df = results_to_dataframe(results)\n",
    "print(df)\n",
    "\n",
    "df.to_csv(\"time_benchmark.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ae8f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"time_benchmark.csv\")\n",
    "df2 = pd.read_csv(\"./Output/time_benchmark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28610b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "df_combined.to_csv(\"./Output/combined_time_benchmark.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b86d84",
   "metadata": {},
   "source": [
    "## GPU Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eeb94379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f550e8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "GPU Name: NVIDIA A100-SXM4-40GB\n",
      "Total memory: 42.47 GB\n",
      "Allocated memory: 0.00 GB\n",
      "Cached memory: 0.05 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "\n",
    "# Get number of GPUs\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Get current GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Get memory info (in bytes)\n",
    "    print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"Cached memory: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36f0ea1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-SXM4-40GB, 40960, 39962, 543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.free,memory.used', \n",
    "                        '--format=csv,noheader,nounits'], \n",
    "                       capture_output=True, text=True)\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebedc769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100-SXM4-40GB\n",
      "  Total memory: 42.95 GB\n",
      "  Free memory: 41.90 GB\n",
      "  Used memory: 1.05 GB\n"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "device_count = pynvml.nvmlDeviceGetCount()\n",
    "\n",
    "for i in range(device_count):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    \n",
    "    print(f\"GPU {i}: {pynvml.nvmlDeviceGetName(handle)}\")\n",
    "    print(f\"  Total memory: {info.total / 1e9:.2f} GB\")\n",
    "    print(f\"  Free memory: {info.free / 1e9:.2f} GB\")\n",
    "    print(f\"  Used memory: {info.used / 1e9:.2f} GB\")\n",
    "\n",
    "pynvml.nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2efac07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content', '/env/python', '/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/usr/local/lib/python3.12/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.12/dist-packages/IPython/extensions', '/root/.ipython', '/tmp/tmpcdghsf86', '/usr/local/lib/python3.12/dist-packages/setuptools/_vendor']\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os \n",
    "print(sys.path) \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a65b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
