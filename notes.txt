## Notes
- ArcTan and ArcTan Approx learns surprisingly well with very low learning rates. Last 50 epochs of 200 epochs with lr=1e-3 with cosine scheduler, it achieves ~60% accuracy on CIFAR-10 with ResNet34. 

## GPT2 WikiText103 Notes
- 32 batch size takes 50,566M or 50.6 GB of memory. 

** GH200 ARM with batch size 40 takes 91,993M or 91.9 GB of memory (total 97.8GB of memory) **
- [0] NVIDIA GH200 480GB | 54Â°C, 100 % | 91993 / 97871 MB | mkang2(91986M) 

- GELU_s needs more memory and need to do batch size = 32

- Learning Rate Warmup is necessary: start with learning rate at 0 and linearly increase it over the first ~1000 steps (or 5% of training)

- fixed lm_head.weight to be token_embeddings weight
- The projection layers must be scaled down by 1/sqrt(2 * layers)

### GPT2 Small low perplexity 
- GPT2 small on WikiText103 with ReLU activation function gets around 15.73 perplexity after 20 epochs of training. 
	- Literature's "37.5" benchmark is usually zero-shot (trained on internet and tested on wikitext)
	- BPE vs. Word-Level - Our model uses GPT-2 Tokenizer (Byte-Pair Encoding -> splits words into chunks)
		- Standard "word-level" benchmarks calculate perplexity per word. 
** 15.73 BPE perplexity is strong & valid baseline for benchmark with GPT2 Small ReLU Activation Function ** 


## Conda Environments
- torch-a100 is able to use amp, compile, and huggingface datasets, tranformers, pytorch, etc. 
- torch-gh200 is not able to use compile (


GPU Information 
- Mixed Pro6000 GPU with 16 CPU does VIT 45 s. per epoch 
- A100 GPU with 4 CPU does VIT 88 s. per epoch 

- How much can we max out the speed for this?? 
- Pro6000 GPU with Max 80 CPU 

[GPT2] 
- With Pro6000 GPU with 80 CPU
- Batch Size : 42 - 48
- Num Workers : 16 
- pin_memory = True
- persistent_workers = True 
- prefetch_factor = 3


[HPC Allocation]
1. Use A100 + 4 CPUS for ResNet + VGG experiments

export OMP_NUM_THREADS=2
export MKL_NUM_THREADS=2
export TORCH_NUM_THREADS=2

	- 2 workers
	- True persistent workers 
	- 2 prefetch factor 
	- True pin memory 
	- 128 - 256 batch size 



2. Use Pro6000 + 80 CPUs for ViT experiments 

export OMP_NUM_THREADS=16
export MKL_NUM_THREADS=16
export TORCH_NUM_THREADS=16

	- 12 - 16 workers 
	- True persistent workers
	- 3 prefetch factor 
	- True pin memory
	- 256-512 batch size 

3.  Use GH200 + 48 CPUs for GPT2 experiments

export OMP_NUM_THREADS=10
export MKL_NUM_THREADS=10
export TORCH_NUM_THREADS=10

	- 8 - 10 num workers 
	- True persistent workers 
	- 2 prefetch factor 
	- True pin memory 
	- 32 - 40 batch size 

BATCH_SIZES = {
    "resnet34": 128,   # All activations use 128
    "vgg16": 128,      # All activations use 128
    "vit": 256,        # All activations use 256
    "gpt2": 40,        # All activations use 40
}

