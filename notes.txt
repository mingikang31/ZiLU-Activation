## Notes
- ArcTan and ArcTan Approx learns surprisingly well with very low learning rates. Last 50 epochs of 200 epochs with lr=1e-3 with cosine scheduler, it achieves ~60% accuracy on CIFAR-10 with ResNet34. 

## GPT2 WikiText103 Notes
- 32 batch size takes 50,566M or 50.6 GB of memory. 

- Learning Rate Warmup is necessary: start with learning rate at 0 and linearly increase it over the first ~1000 steps (or 5% of training)

- fixed lm_head.weight to be token_embeddings weight
- The projection layers must be scaled down by 1/sqrt(2 * layers)

## Conda Environments
- torch-a100 is able to use amp, compile, and huggingface datasets, tranformers, pytorch, etc. 